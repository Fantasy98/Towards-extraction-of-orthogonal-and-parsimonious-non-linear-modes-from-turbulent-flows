# -*- coding: utf-8 -*-
"""CNN_VAE_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApImr2iYestX5Oy4ektG76ueeBg8VGPZ
"""

import numpy as np
from scipy.io import loadmat
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses
from tensorflow.keras import backend as K
from scipy.io import savemat

from google.colab import drive
drive.mount("/content/drive",  force_remount=True)

wdir = '/content/drive/My Drive/Urban_Flow/'

dirc = '/content/drive/My Drive/Urban_Flow/'

d = loadmat(dirc + 'Y100_T7to12_all.mat')
u = d['Y100_T7to12_all'][:, :96, 4:196, 0:1]


u_mean = u.mean(axis = 0)
u -= u_mean

nt, nx, ny, nv = u.shape

rank = 'f16_r10_Y100_7to12_U_B1e-3'
model_name = f'_{rank}.h5'

ind_val = np.zeros(nt, dtype = bool)
n_val = int(nt * 0.2)
ind_val[:n_val] = True
np.random.seed(24)
np.random.shuffle(ind_val)

u_trn = u[~ind_val]
u_tst = u[ind_val]

latent_dim = 10
def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                              mean=0., stddev=1.0)
    return z_mean + K.exp(z_log_sigma) * epsilon

act = 'tanh'
fs = (3, 3)
ffs = (2, 2)
st = (1, 1)
inp = layers.Input(shape = (nx, ny, nv))
x = layers.Conv2D(16, fs, activation = act, strides = st, padding='same')(inp)
x = layers.MaxPooling2D(ffs, padding = 'same')(x)
x = layers.Conv2D(32, fs, activation = act, strides = st, padding='same')(x)
x = layers.MaxPooling2D(ffs, padding = 'same')(x)
x = layers.Conv2D(64, fs, activation = act, strides = st, padding='same')(x)
x = layers.MaxPooling2D(ffs, padding = 'same')(x)
x = layers.Conv2D(128, fs, activation = act, strides = st, padding='same')(x)
x = layers.MaxPooling2D(ffs, padding = 'same')(x)
x = layers.Conv2D(256, fs, activation = act, strides = st, padding='same')(x)
x = layers.MaxPooling2D(ffs, padding = 'same')(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation=act)(x)
z_mean = layers.Dense(latent_dim)(x)
z_log_sigma = layers.Dense(latent_dim)(x)
z = layers.Lambda(sampling)([z_mean, z_log_sigma])

encoder = models.Model(inp, [z_mean, z_log_sigma, z])
print(encoder.summary())

code_i = layers.Input(shape = (latent_dim,))
x = layers.Dense(128, activation=act)(code_i)
x = layers.Dense(4608, activation = act)(x)
x = layers.Reshape((3, 6, 256))(x)
x = layers.UpSampling2D(ffs)(x)
x = layers.Conv2D(256, fs, activation = act, strides = st, padding='same')(x)
x = layers.UpSampling2D(ffs)(x)
x = layers.Conv2D(128, fs, activation = act, strides = st, padding='same')(x)
x = layers.UpSampling2D(ffs)(x)
x = layers.Conv2D(64, fs, activation = act, strides = st, padding='same')(x)
x = layers.UpSampling2D(ffs)(x)
x = layers.Conv2D(32, fs, activation = act, strides = st, padding='same')(x)
x = layers.UpSampling2D(ffs)(x)
x = layers.Conv2D(16, fs, activation = act, strides = st, padding='same')(x)
out = layers.Conv2D(nv, fs, strides = st, padding='same')(x)

decoder = models.Model(code_i, out)
print(decoder.summary())

out_d = decoder(encoder(inp)[2])

model = models.Model(inp, out_d)
print(model.summary())

rec_loss = losses.mse(K.reshape(inp, (-1,)), K.reshape(out_d, (-1,)))
kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -0.5
# Here the beta is set to 1e-3
vae_loss = K.mean(rec_loss + 1e-3 * kl_loss)
model.add_loss(vae_loss)
model.add_metric(rec_loss, name='rec_loss', aggregation='mean')
model.add_metric(kl_loss, name='kl_loss', aggregation='mean')

step = tf.Variable(0, trainable=False)
boundaries = [500, 100, 100]
values = [1e-3, 1e-4, 1e-5, 1e-6]
lr = optimizers.schedules.PiecewiseConstantDecay(boundaries, values)

opt = optimizers.Adam(learning_rate = lr(step))
model.compile(optimizer = opt, loss = 'mse')
hist = model.fit(u_trn, u_trn, epochs = 800, validation_data = (u_tst, u_tst), verbose = 2) 
savemat(wdir + f'CNNVAEloss_{rank}.mat', hist.history)

encoder.save(wdir + 'CNNVAEen' + model_name)
decoder.save(wdir + 'CNNVAEde' + model_name)

u_p = model.predict(u)
plt.imshow(u[0, :, :, 0])
plt.figure()
plt.imshow(u_p[0, :, :, 0])

c = encoder.predict(u)

plt.figure()
coef = np.abs(np.corrcoef(c[0].T))
plt.imshow(coef, cmap = 'RdBu_r', vmin = 0, vmax = 1)

cm = np.diag(np.ones((latent_dim,)))
modes = decoder.predict(cm)

for n in range(latent_dim):
  plt.figure()
  plt.imshow(modes[n, :, :, 0], cmap = 'RdBu')

plt.plot(c[0])